<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Music Style Transfer - Datasets </title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="../assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="../assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">

	<!-- Header -->
	<header id="header">
		<a href="../index.html" class="title">Music Style Transfer</a>
		<nav>
			<ul>
				<li><a href="../index.html">Home</a></li>
				<li><a href="project.html">Introduction</a></li>
				<li><a href="resources.html" class="active">Resources</a></li>
				<li><a href="listen.html">Listen</a></li>
			</ul>
		</nav>
	</header>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<section id="main" class="wrapper">
			<div class="inner">
				<h2 class="major">Abstract</h2>
				<p>While still in its infancy, the field of Music Style Transfer has seen various approaches to solving
					the problem of converting a piece of music from one genre to another.
					From variational autoencoders and generative adversarial networks to supervised learning methods,
					researchers have had some success in solving this problem, yet due to the subjective nature of the
					field, there is no consensus on how to compare the results of the various models.
					Added to this, is the fact that some of the models have not been maintained and use deprecated
					libraries, making them harder to assess.
					In this dissertation, we refactor the existing code base of two existing models and use different
					approaches for processing the data, while maintaining model architecture frozen, and evaluate their
					performance using metrics proposed by newly published research.
					The results allow us to draw insight into the approaches that generates better results of music
					style transfer.
				</p>
				<h1 class="major">Code</h1>
				<h2 class="major">ChordGAN</h2>
				<section>
					<p>A simple model that uses fully connected layers to process the MIDI input and the chromagram
						representation.</p>
					<a href="https://github.com/amaralcs/ChordGAN-v2">Link to ChordGAN repo.</a>
				</section>

				<h2 class="major">CycleGAN</h2>
				<section>
					<p>A deep convolutional network that employs two generators, two discriminators and custom loss
						functions.</p>
					<a href="https://github.com/amaralcs/tf-cyclegan-music-style-transfer">Link to CycleGAN repo.</a>
				</section>

				<h1 class="major">Datasets</h1>

				<!-- Text -->
				<section>
					<p>The models evaluated use the MIDI format and we used the datasets provided by ChordGAN and
						CycleGAN.</p>
					<p>Datasets can be found on <a href="https://zenodo.org/record/6959362">Zenodo</a>. </p>
				</section>

				<!-- Footer -->
				<footer id="footer" class="wrapper alt">
					<div class="inner">
						<ul class="menu">
							<li>&copy; Untitled. All rights reserved.</li>
							<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</div>
				</footer>

				<!-- Scripts -->
				<script src="../assets/js/jquery.min.js"></script>
				<script src="../assets/js/jquery.scrollex.min.js"></script>
				<script src="../assets/js/jquery.scrolly.min.js"></script>
				<script src="../assets/js/browser.min.js"></script>
				<script src="../assets/js/breakpoints.min.js"></script>
				<script src="../assets/js/util.js"></script>
				<script src="../assets/js/main.js"></script>


</body>

</html>